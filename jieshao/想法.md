论文想法

# 题目拟定

基于语音和面部特征融合的情绪识别

人脸与语音的多模态情绪识别

# 整体脉络

**1.人脸情绪识别**

多模型训练比较，调整参数，得出最优模型参数

（加入新模型进行比较？，人脸算法优化--遮挡？）

例如：预处理时使用MTCNN进行更精确的处理

**更准确的面部检测**：MTCNN 在面部检测方面表现优秀，能够更准确地裁剪出面部区域，这对于情感识别模型是非常关键的。精确的面部区域输入可以减少背景噪声，提高模型的准确性。

**面部对齐（Alignment）**：MTCNN 提供了面部关键点检测的功能，通过对眼睛、鼻子和嘴巴等位置进行检测，可以将面部进行对齐。对齐后的面部图像可以减少由于角度、表情等造成的差异，使得模型能够更专注于面部的情感特征。

**减少数据中的偏差**：在很多面部情感数据集中，面部可能会有不同角度或不规则的裁剪。MTCNN可以使面部统一到相同的位置和尺度，从而减少模型在训练时对这些不相关信息的敏感度。

如何集成到现有项目?

1)**安装 MTCNN**： 你可以使用 `facenet-pytorch` 库

2)**在数据预处理阶段使用 MTCNN**：

```python
from mtcnn import MTCNN  # 添加MTCNN导入
import os
import shutil
import csv
import cv2

import source.config as config
import source.face_emotion_utils.face_config as face_config

detector = MTCNN()  # 初始化MTCNN面部检测器

def convert_RAF_data(folder_path, save_path, label_file_path):
    def get_label_schame(file_label_dict, file_name, s_no):
        label = file_label_dict[file_name]
        emotion = EMOTION_INDEX_RAF[label]
        label = f"RAF_{s_no}_{emotion}.jpg"
        return label

    def get_file_label_dict(label_file_path):
        file_label_dict = {}
        with open(label_file_path, 'r') as f:
            for line in f:
                line = line.split(" ")
                file_label_dict[line[0]] = int(line[1])
        return file_label_dict

    EMOTION_INDEX_RAF = {
        6: 'Angry',
        3: 'Disgust',
        2: 'Fear',
        4: 'Happy',
        7: 'Neutral',
        5: 'Sad',
        1: 'Surprise',
    }

    file_label_dict = get_file_label_dict(label_file_path)

    cnt = 0
    for i, file in enumerate(os.listdir(folder_path)):
        print("Converting " + file)
        file_path = os.path.join(folder_path, file)

        # 读取图像
        image = cv2.imread(file_path)

        # 使用MTCNN进行面部检测 - 修改部分
        results = detector.detect_faces(image)
        if len(results) == 0:
            print(f"No face detected in {file_path}")
            continue

        # 裁剪检测到的面部 - 修改部分
        x, y, width, height = results[0]['box']
        face = image[y:y + height, x:x + width]

        # 保存裁剪后的面部图像
        face_save_path = save_path + get_label_schame(file_label_dict, file, cnt)
        cv2.imwrite(face_save_path, face)
        cnt += 1

    print("Conversion complete")

```



```python
def preprocess_images(
        original_images_folders=config.ALL_EXTRACTED_FACES_FOLDERS,
        output_path=config.PREPROCESSED_IMAGES_FOLDER_PATH,
        print_flag=True,
):
    all_face_land_dists_depths_X = []  # List of all the face landmarks distances and depths
    all_face_images_X = []  # List of all images
    all_face_emotions_Y = []  # List of all the emotions as softmax

    all_cnt = 0
    for folder in original_images_folders:
        for file in os.listdir(folder):
            all_cnt += 1

    detected_cnt = 1
    so_far_cnt = 0
    for folder in original_images_folders:
        for file in os.listdir(folder):
            if print_flag:
                so_far_cnt += 1
                print(f"\nPreprocessing file {so_far_cnt}/{all_cnt}: {folder.split(config.ls)[-2]}/{file}")
                print(f"Detected cnt: {detected_cnt}/{so_far_cnt}")

            #image = cv2.imread(folder + config.ls + file) //优化代码
            image_path = folder + config.ls + file
            image = cv2.imread(image_path)
            
            
            results = detector.detect_faces(image)
            if len(results) == 0:  # 没有检测到面部
                if print_flag:
                    print("No face detected")
                    print(".....SKIPPING FILE", file)
                continue

            # 检测到面部后进行裁剪 - 修改部分
            x, y, width, height = results[0]['box']
            face = image[y:y + height, x:x + width]

            # 转换为灰度图 - 修改部分
            if len(face.shape) > 2:
                grey_image = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)
            else:
                grey_image = face
            grey_image = cv2.resize(grey_image, (face_config.FACE_SIZE, face_config.FACE_SIZE))


            # 获取面部情绪标签
            emotion_label = config.FULL_EMOTION_INDEX_REVERSE[file.split("_")[2].split(".")[0]]
            emotion_label_softmax = utils.get_as_softmax(emotion_label, config.NON_SIMPLIFIED_SOFTMAX_LEN)

            if print_flag:
                print(f"Image shape: {grey_image.shape}")
                print(f"Emotion label softmax: {emotion_label}, {emotion_label_softmax}")

            # 添加到列表
            all_face_land_dists_depths_X.append(land_dists)
            all_face_images_X.append(grey_image)
            all_face_emotions_Y.append(emotion_label_softmax)
            detected_cnt += 1


    print("Saving final data to file")
    # Save remaining data
    save_preprocessed_data(
        all_face_land_dists_depths_X,
        all_face_images_X,
        all_face_emotions_Y,
        output_path=output_path,
        save_name_suffix="_default",
    )
    print("Preprocessing complete")

    return all_face_land_dists_depths_X, all_face_images_X, all_face_emotions_Y
```

  **使用 MTCNN 的潜在提升：**

- **准确度提高**：由于 MTCNN 能够提供更准确的面部裁剪和对齐，模型有可能对情感特征的捕捉更加精确，从而提升情感分类的准确度。
- **鲁棒性增强**：在面对角度变化、不同大小或部分遮挡的面部图像时，MTCNN 有助于提高模型的鲁棒性。
- **更好的泛化能力**：通过标准化面部输入，模型在训练过程中会更容易找到有效的情感特征，从而提升在不同数据集或真实场景下的泛化能力。

引入自注意力机制：

1）添加自注意力机制模块

可以在`model.py`中定义一个自注意力模块。典型的自注意力机制会计算输入的特征图的**Query**、**Key**和**Value**，然后通过注意力分数对输入特征进行加权。

```python
import torch
import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, in_dim):
        super(SelfAttention, self).__init__()
        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        batch_size, C, width, height = x.size()
        query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)  # B x (W*H) x C
        key = self.key_conv(x).view(batch_size, -1, width * height)  # B x C x (W*H)
        value = self.value_conv(x).view(batch_size, -1, width * height)  # B x C x (W*H)

        attention = torch.bmm(query, key)  # B x (W*H) x (W*H)
        attention = torch.nn.functional.softmax(attention, dim=-1)  # softmax

        out = torch.bmm(value, attention.permute(0, 2, 1))  # B x C x (W*H)
        out = out.view(batch_size, C, width, height)

        out = self.gamma * out + x
        return out
```

2）修改模型架构

在`CustomModel`类中，添加自注意力机制模块。你可以选择在卷积特征提取之后添加它。可以将`SelfAttention`层插入到卷积层和全连接层之间，以加强模型的注意力机制。

```python
class CustomModel(CustomModelBase):
    def __init__(
            self,
            input_shapes,
            dropout_rate,
            dense_units,
            num_layers,
            l1_l2_reg,
            layers_batch_norm,
            conv_model_name,
            use_landmarks,
            class_weights=None,
            device=device,
    ):
        # 初始化卷积模型和全连接层
        super(CustomModel, self).__init__(class_weights=class_weights)
        self.use_landmarks = use_landmarks
        input_shape, input_shape_2 = input_shapes

        # 初始化预训练的卷积模型
        self.base_model_conv = self.get_conv_model(conv_model_name, pretrained=True)
        self.base_model = nn.Sequential(*list(self.base_model_conv.children())[:-1])

        # 添加自注意力机制
        self.self_attention = SelfAttention(in_dim=self.base_model[-1].out_channels)

        self.flatten = nn.Flatten()
        with torch.no_grad():
            sample_input = torch.randn(1, input_shape[0], input_shape[1], input_shape[2])
            self.base_output_size = self.base_model(sample_input).numel()

        dense_input_size = self.base_output_size + input_shape_2

        dense_layers = []
        for _ in range(num_layers - 1):
            dense_layer = [
                nn.Linear(dense_units, dense_units),
                nn.ReLU(),
                nn.Dropout(dropout_rate)
            ]
            if layers_batch_norm:
                dense_layer.append(nn.BatchNorm1d(dense_units))
            dense_layers.extend(dense_layer)

        self.fc = nn.Sequential(
            nn.Linear(dense_input_size, dense_units),
            nn.BatchNorm1d(dense_units),
            *dense_layers,
        )

        self.out_lands = nn.Linear(dense_units, SOFTMAX_LEN)
        self.out_no_lands = nn.Linear(self.base_output_size, SOFTMAX_LEN)

    def forward(self, x1, x2):
        # 卷积特征提取
        x1 = self.base_model(x1)
        
        # 自注意力机制
        x1 = self.self_attention(x1)
        
        x1 = self.flatten(x1)

        if self.use_landmarks:
            x = torch.cat((x1, x2), dim=1)
            x = self.fc(x)
            x = self.out_lands(x)
        else:
            x = self.out_no_lands(x1)

        return x

```

整体修改为：

```
import torch
import torch.nn as nn
import torch.nn.functional as F
import source.pytorch_utils.training_utils as pt_train
# 省略其他 import

# (修改部分) 定义自注意力模块
class SelfAttention(nn.Module):
    def __init__(self, in_dim):
        super(SelfAttention, self).__init__()
        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        batch_size, C, width, height = x.size()
        query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)  # B x (W*H) x C
        key = self.key_conv(x).view(batch_size, -1, width * height)  # B x C x (W*H)
        value = self.value_conv(x).view(batch_size, -1, width * height)  # B x C x (W*H)

        attention = torch.bmm(query, key)  # B x (W*H) x (W*H)
        attention = torch.nn.functional.softmax(attention, dim=-1)  # softmax

        out = torch.bmm(value, attention.permute(0, 2, 1))  # B x C x (W*H)
        out = out.view(batch_size, C, width, height)

        out = self.gamma * out + x
        return out


class CustomModelBase(pt_train.CustomModelBase):
    def __init__(self, class_weights):
        super(CustomModelBase, self).__init__()
        self.class_weights = class_weights

    def training_step(self, batch):
        images, lands, labels = batch
        out = self(images, lands)
        loss = F.cross_entropy(out, labels, weight=self.class_weights)
        acc = pt_train.accuracy(out, labels)
        return loss, acc

    def validation_step(self, batch):
        images, lands, labels = batch
        out = self(images, lands)
        loss = F.cross_entropy(out, labels, weight=self.class_weights)
        acc = pt_train.accuracy(out, labels)
        return {'val_loss': loss.detach(), 'val_acc': acc}

# (修改部分) 修改后的 CustomModel 类
class CustomModel(CustomModelBase):
    def __init__(
            self,
            input_shapes,
            dropout_rate,
            dense_units,
            num_layers,
            l1_l2_reg,
            layers_batch_norm,
            conv_model_name,
            use_landmarks,
            class_weights=None,
            device=device,
    ):
        if class_weights is None:
            class_weights = torch.ones(SOFTMAX_LEN)
        else:
            class_weights = torch.tensor(class_weights)

        class_weights = class_weights.to(device)
        super(CustomModel, self).__init__(class_weights=class_weights)

        self.use_landmarks = use_landmarks

        input_shape, input_shape_2 = input_shapes
        self.base_model_conv = self.get_conv_model(conv_model_name, pretrained=True)
        self.base_model = nn.Sequential(*list(self.base_model_conv.children())[:-1])

        # (修改部分) 添加自注意力机制
        self.self_attention = SelfAttention(in_dim=self.base_model[-1].out_channels)

        self.flatten = nn.Flatten()

        with torch.no_grad():
            sample_input = torch.randn(1, input_shape[0], input_shape[1], input_shape[2])
            self.base_output_size = self.base_model(sample_input).numel()

        dense_input_size = self.base_output_size + input_shape_2

        dense_layers = []
        for _ in range(num_layers - 1):
            dense_layer = [
                nn.Linear(dense_units, dense_units),
                nn.ReLU(),
                nn.Dropout(dropout_rate)
            ]
            if layers_batch_norm:
                dense_layer.append(nn.BatchNorm1d(dense_units))
            dense_layers.extend(dense_layer)

        self.fc = nn.Sequential(
            nn.Linear(dense_input_size, dense_units),
            nn.BatchNorm1d(dense_units),
            *dense_layers,
        )

        self.out_lands = nn.Linear(dense_units, SOFTMAX_LEN)
        self.out_no_lands = nn.Linear(self.base_output_size, SOFTMAX_LEN)

    def get_conv_model(self, conv_model_name, pretrained=True):
        if conv_model_name == "resnet50":
            return resnet50(pretrained=pretrained)
        # 省略其他模型部分
        else:
            raise ValueError("Invalid model name, exiting...")

    def forward(self, x1, x2):
        x1 = self.base_model(x1)

        # (修改部分) 使用自注意力模块
        x1 = self.self_attention(x1)

        x1 = self.flatten(x1)

        if self.use_landmarks:
            x = torch.cat((x1, x2), dim=1)
            x = self.fc(x)
            x = self.out_lands(x)
        else:
            x = self.out_no_lands(x1)

        return x

```

**2.语音情绪识别**

多模型比较，得出最优参数

**3.人脸+语音融合情绪识别**

将最优参数结合，通过自定义模型载入两个模型数据，训练得出新模型